{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdb6f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Dict, Optional, List\n",
    "import os\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import queue\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "from contextlib import nullcontext\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from typing import Union, List, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor, nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data._utils.worker import ManagerWatchdog\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification, AutoModel, is_torch_npu_available\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Qwen3Reranker:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name_or_path: str,\n",
    "        max_length: int = 4096,\n",
    "        instruction=None,\n",
    "        attn_type='causal',\n",
    "    ) -> None:\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "        self.max_length=max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, padding_side='left')\n",
    "        self.lm = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float16).cuda().eval()\n",
    "        self.token_false_id = self.tokenizer.convert_tokens_to_ids(\"no\")\n",
    "        self.token_true_id = self.tokenizer.convert_tokens_to_ids(\"yes\")\n",
    "\n",
    "        self.prefix = \"<|im_start|>system\\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \\\"yes\\\" or \\\"no\\\".<|im_end|>\\n<|im_start|>user\\n\"\n",
    "        self.suffix = \"<|im_end|>\\n<|im_start|>assistant\\n<think>\\n\\n</think>\\n\\n\"\n",
    "\n",
    "        self.prefix_tokens = self.tokenizer.encode(self.prefix, add_special_tokens=False)\n",
    "        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)\n",
    "        self.instruction = instruction\n",
    "        if self.instruction is None:\n",
    "            self.instruction = \"Given the user query, retrieval the relevant passages\"\n",
    "\n",
    "    def format_instruction(self, instruction, query, doc):\n",
    "        if instruction is None:\n",
    "            instruction = self.instruction\n",
    "        output = \"<Instruct>: {instruction}\\n<Query>: {query}\\n<Document>: {doc}\".format(instruction=instruction,query=query, doc=doc)\n",
    "        return output\n",
    "\n",
    "    def process_inputs(self, pairs):\n",
    "        out = self.tokenizer(\n",
    "            pairs, padding=False, truncation='longest_first',\n",
    "            return_attention_mask=False, max_length=self.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)\n",
    "        )\n",
    "        for i, ele in enumerate(out['input_ids']):\n",
    "            out['input_ids'][i] = self.prefix_tokens + ele + self.suffix_tokens\n",
    "        out = self.tokenizer.pad(out, padding=True, return_tensors=\"pt\", max_length=self.max_length)\n",
    "        for key in out:\n",
    "            out[key] = out[key].to(self.lm.device)\n",
    "        return out\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_logits(self, inputs, **kwargs):\n",
    "\n",
    "        batch_scores = self.lm(**inputs).logits[:, -1, :]\n",
    "        true_vector = batch_scores[:, self.token_true_id]\n",
    "        false_vector = batch_scores[:, self.token_false_id]\n",
    "        batch_scores = torch.stack([false_vector, true_vector], dim=1)\n",
    "        batch_scores = torch.nn.functional.log_softmax(batch_scores, dim=1)\n",
    "        scores = batch_scores[:, 1].exp().tolist()\n",
    "        return scores\n",
    "\n",
    "    def compute_scores(\n",
    "        self,\n",
    "        pairs,\n",
    "        instruction=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        pairs = [self.format_instruction(instruction, query, doc) for query, doc in pairs]\n",
    "        inputs = self.process_inputs(pairs)\n",
    "        scores = self.compute_logits(inputs)\n",
    "        return scores\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = Qwen3Reranker(model_name_or_path='../../git/model/Qwen3-Reranker-0.6B', instruction=\"Retrieval document that can answer user's query\", max_length=2048)\n",
    "    queries = ['What is the capital of China?', 'Explain gravity']\n",
    "    documents = [\n",
    "        \"The capital of China is Beijing.\",\n",
    "        \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n",
    "    ]\n",
    "    pairs = list(zip(queries, documents))\n",
    "    instruction=\"Given the user query, retrieval the relevant passages\"\n",
    "    new_scores = model.compute_scores(pairs, instruction)\n",
    "    print('scores', new_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bd5870",
   "metadata": {},
   "source": [
    "- 输入必须成对\n",
    "- 设计prompt引导模型回答yes,no,然后提取模型yes,no的token的概率,从而得到相关性,注意,最后只取了yes的概率"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
