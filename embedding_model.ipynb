{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df114a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\llm\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\anaconda\\envs\\llm\\lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#coding:utf8\n",
    "import os\n",
    "from typing import Dict, Optional, List, Union\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class Qwen3Embedding():\n",
    "    def __init__(self, model_name_or_path, instruction=None,  use_fp16: bool = True, use_cuda: bool = True, max_length=8192):\n",
    "        if instruction is None:\n",
    "            instruction = 'Given a web search query, retrieve relevant passages that answer the query'\n",
    "        self.instruction = instruction\n",
    "        if is_flash_attn_2_available() and use_cuda:\n",
    "            self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)\n",
    "        else:\n",
    "            self.model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.float16)\n",
    "        if use_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True, padding_side='right')\n",
    "        self.max_length=max_length\n",
    "    \n",
    "    def last_token_pool(self, last_hidden_states: Tensor,\n",
    "        attention_mask: Tensor) -> Tensor:\n",
    "        left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "        if left_padding:\n",
    "            return last_hidden_states[:, -1]\n",
    "        else:\n",
    "            sequence_lengths = attention_mask.sum(dim=1) - 1\n",
    "            batch_size = last_hidden_states.shape[0]\n",
    "        return last_hidden_states[torch.arange(batch_size, device=last_hidden_states.device), sequence_lengths]\n",
    "\n",
    "    def get_detailed_instruct(self, task_description: str, query: str) -> str:\n",
    "        if task_description is None:\n",
    "            task_description = self.instruction\n",
    "        return f'Instruct: {task_description}\\nQuery:{query}'\n",
    "\n",
    "    def encode(self, sentences: Union[List[str], str], is_query: bool = False, instruction=None, dim: int = -1):\n",
    "        if isinstance(sentences, str):\n",
    "            sentences = [sentences]\n",
    "        if is_query:\n",
    "            sentences = [self.get_detailed_instruct(instruction, sent) for sent in sentences]\n",
    "        inputs = self.tokenizer(sentences, padding=True, truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "        inputs.to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            model_outputs = self.model(**inputs)\n",
    "            output = self.last_token_pool(model_outputs.last_hidden_state, inputs['attention_mask'])\n",
    "            if dim != -1:\n",
    "                output = output[:, :dim]\n",
    "            output  = F.normalize(output, p=2, dim=1)\n",
    "        return output\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"../../git/model/Qwen3-Embedding-0.6B\"\n",
    "    model = Qwen3Embedding(model_path)\n",
    "    queries = ['What is the capital of China?', 'Explain gravity']\n",
    "    documents = [\n",
    "        \"The capital of China is Beijing.\",\n",
    "        \"Gravity is a force that attracts two bodies towards each other. It gives weight to physical objects and is responsible for the movement of planets around the sun.\"\n",
    "    ]\n",
    "    dim = 1024\n",
    "    query_outputs = model.encode(queries, is_query=True, dim=dim)\n",
    "    doc_outputs = model.encode(documents, dim=dim)\n",
    "    print('query outputs', query_outputs)\n",
    "    print('doc outputs', doc_outputs)\n",
    "    scores = (query_outputs @ doc_outputs.T) * 100\n",
    "    print(scores.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3cd018",
   "metadata": {},
   "source": [
    "- 注意左右padding不一致,最后选择last_hidden_states的方式不同\n",
    "- 是query,进行instruction的填充,应该是为了提高检索准确性\n",
    "- 巧妙地判断是左padding还是右padding\n",
    "    - left_padding = (attention_mask[:, -1].sum() == attention_mask.shape[0])\n",
    "- 最后地相似度，通过点积计算、\n",
    "    - scores = (query_outputs @ doc_outputs.T)\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
